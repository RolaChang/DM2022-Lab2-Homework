{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Mining Lab 2\nIn this lab session we will focus on the use of Neural Word Embeddings ","metadata":{"id":"uuutyCx4YTpX"}},{"cell_type":"markdown","source":"---\n# Introduction","metadata":{"id":"LIpAqCvMYTpX"}},{"cell_type":"markdown","source":"**Dataset:** [SemEval 2017 Task](https://competitions.codalab.org/competitions/16380)\n\n**Task:** Classify text data into 4 different emotions using word embedding and other deep information retrieval approaches.\n\n![image.png](https://raw.githubusercontent.com/keziatamus/DM2022-Lab2-Master/08755efc671824064d7a9347edb8c418550c3e83//pics/pic0.png)","metadata":{"id":"n2paPeNbYTpX"}},{"cell_type":"markdown","source":"---\n# 1. Data Preparation","metadata":{"id":"op_X7pR-YTpX"}},{"cell_type":"markdown","source":"Before beggining the lab, please make sure to download the [Google News Dataset](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit) and place it in a folder named \"GoogleNews\" in the same directory as this file.","metadata":{"id":"ID-8I1ELYTpX"}},{"cell_type":"markdown","source":"## 1.1 Load data\n\nWe start by loading the csv files into a single pandas dataframe for training and one for testing.","metadata":{"id":"pgoEbZzSYTpX"}},{"cell_type":"code","source":"# import library\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport gensim\nimport tensorflow\nimport keras\n%matplotlib inline\nimport plotly.offline as pyo\nimport plotly.express as px\nimport plotly.graph_objects as go\npyo.init_notebook_mode()\n\nprint(\"gensim: \" + gensim.__version__)\nprint(\"tensorflow: \" + tensorflow.__version__)\nprint(\"keras: \" + keras.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:17:04.683744Z","iopub.execute_input":"2022-11-22T01:17:04.684742Z","iopub.status.idle":"2022-11-22T01:17:15.396114Z","shell.execute_reply.started":"2022-11-22T01:17:04.684617Z","shell.execute_reply":"2022-11-22T01:17:15.395177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n### training data\nanger_train = pd.read_csv(\"../input/lab2-dataset/data/semeval/train/anger-ratings-0to1.train.txt\",\n                         sep=\"\\t\", header=None,names=[\"id\", \"text\", \"emotion\", \"intensity\"])\nsadness_train = pd.read_csv(\"../input/lab2-dataset/data/semeval/train/sadness-ratings-0to1.train.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\nfear_train = pd.read_csv(\"../input/lab2-dataset/data/semeval/train/fear-ratings-0to1.train.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\njoy_train = pd.read_csv(\"../input/lab2-dataset/data/semeval/train/joy-ratings-0to1.train.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])","metadata":{"id":"anfjcPSSYTpX","execution":{"iopub.status.busy":"2022-11-22T01:17:28.834612Z","iopub.execute_input":"2022-11-22T01:17:28.835039Z","iopub.status.idle":"2022-11-22T01:17:28.908349Z","shell.execute_reply.started":"2022-11-22T01:17:28.835001Z","shell.execute_reply":"2022-11-22T01:17:28.907036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combine 4 sub-dataset\ntrain_df = pd.concat([anger_train, fear_train, joy_train, sadness_train], ignore_index=True)","metadata":{"id":"yVc2T5MIYTpX","execution":{"iopub.status.busy":"2022-11-22T01:17:31.047778Z","iopub.execute_input":"2022-11-22T01:17:31.048798Z","iopub.status.idle":"2022-11-22T01:17:31.059407Z","shell.execute_reply.started":"2022-11-22T01:17:31.048749Z","shell.execute_reply":"2022-11-22T01:17:31.057799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### testing data\nanger_test = pd.read_csv(\"../input/lab2-dataset/data/semeval/dev/anger-ratings-0to1.dev.gold.txt\",\n                         sep=\"\\t\", header=None,names=[\"id\", \"text\", \"emotion\", \"intensity\"])\nsadness_test = pd.read_csv(\"../input/lab2-dataset/data/semeval/dev/sadness-ratings-0to1.dev.gold.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\nfear_test = pd.read_csv(\"../input/lab2-dataset/data/semeval/dev/fear-ratings-0to1.dev.gold.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\njoy_test = pd.read_csv(\"../input/lab2-dataset/data/semeval/dev/joy-ratings-0to1.dev.gold.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n\n# combine 4 sub-dataset\ntest_df = pd.concat([anger_test, fear_test, joy_test, sadness_test], ignore_index=True)\ntrain_df.head()","metadata":{"id":"Kw8bGMv7YTpX","outputId":"9f6f7052-302e-4794-ef69-b84450b61b36","execution":{"iopub.status.busy":"2022-11-22T01:17:34.622138Z","iopub.execute_input":"2022-11-22T01:17:34.622562Z","iopub.status.idle":"2022-11-22T01:17:34.688124Z","shell.execute_reply.started":"2022-11-22T01:17:34.622531Z","shell.execute_reply":"2022-11-22T01:17:34.686863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shuffle dataset\ntrain_df = train_df.sample(frac=1)\ntest_df = test_df.sample(frac=1)","metadata":{"id":"HBHwcL8sYTpX","execution":{"iopub.status.busy":"2022-11-22T01:17:37.989051Z","iopub.execute_input":"2022-11-22T01:17:37.989500Z","iopub.status.idle":"2022-11-22T01:17:38.000245Z","shell.execute_reply.started":"2022-11-22T01:17:37.989438Z","shell.execute_reply":"2022-11-22T01:17:37.999224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of Training df: \", train_df.shape)\nprint(\"Shape of Testing df: \", test_df.shape)","metadata":{"id":"9w_cDUwCYTpX","outputId":"3582ac44-1f5f-4cb2-b833-d477f152461a","scrolled":true,"execution":{"iopub.status.busy":"2022-11-22T01:17:43.441755Z","iopub.execute_input":"2022-11-22T01:17:43.442199Z","iopub.status.idle":"2022-11-22T01:17:43.447302Z","shell.execute_reply.started":"2022-11-22T01:17:43.442123Z","shell.execute_reply":"2022-11-22T01:17:43.446428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 1 (Take home): **  \nPlot word frequency for Top 30 words in both train and test dataset. (Hint: refer to DM lab 1)\n","metadata":{"id":"escCgU1zYTpX"}},{"cell_type":"code","source":"# Answer here\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount_vect = CountVectorizer(max_features=30)\ntrain_df_counts = count_vect.fit_transform(train_df.text) #(3613, 30)\ntest_df_counts = count_vect.fit_transform(test_df.text)   #(347, 30)\n\ndef Plot_word_frequency(matrix):\n    feature_names =[]\n    for j in range(30):\n        feature_names.append(count_vect.get_feature_names()[j])\n\n    frequency =np.asarray(matrix.sum(axis=0))[0]\n    g = px.bar(x=frequency, y=feature_names).update_yaxes(categoryorder = 'total descending')\n    g.show()","metadata":{"id":"HoXjet3pYTpo","execution":{"iopub.status.busy":"2022-11-22T01:17:46.656257Z","iopub.execute_input":"2022-11-22T01:17:46.657243Z","iopub.status.idle":"2022-11-22T01:17:46.768476Z","shell.execute_reply.started":"2022-11-22T01:17:46.657197Z","shell.execute_reply":"2022-11-22T01:17:46.766962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot word frequency for Top 30 words in train dataset.\nPlot_word_frequency(train_df_counts)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:17:49.455712Z","iopub.execute_input":"2022-11-22T01:17:49.457085Z","iopub.status.idle":"2022-11-22T01:17:50.563362Z","shell.execute_reply.started":"2022-11-22T01:17:49.457030Z","shell.execute_reply":"2022-11-22T01:17:50.561970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot word frequency for Top 30 words in test dataset.\nPlot_word_frequency(test_df_counts)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:17:53.639606Z","iopub.execute_input":"2022-11-22T01:17:53.640013Z","iopub.status.idle":"2022-11-22T01:17:53.707223Z","shell.execute_reply.started":"2022-11-22T01:17:53.639980Z","shell.execute_reply":"2022-11-22T01:17:53.705871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 2 (Take home): **  \nGenerate an embedding using the TF-IDF vectorizer instead of th BOW one with 1000 features and show the feature names for features [100:110].","metadata":{"id":"fj6TV4ngYTpo"}},{"cell_type":"code","source":"# Answer here\nTFIDF_1000 = TfidfVectorizer(max_features=1000, tokenizer=nltk.word_tokenize)\nTFIDF_1000.fit(train_df['text'])\ntrain_data_TFIDF_1000 = TFIDF_1000.transform(train_df['text'])\n\ntrain_data_TFIDF_1000.shape","metadata":{"id":"BOjVbgmxYTpo","execution":{"iopub.status.busy":"2022-11-22T01:19:30.574260Z","iopub.execute_input":"2022-11-22T01:19:30.574710Z","iopub.status.idle":"2022-11-22T01:19:32.610597Z","shell.execute_reply.started":"2022-11-22T01:19:30.574662Z","shell.execute_reply":"2022-11-22T01:19:32.609329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TFIDF_1000.get_feature_names_out()[100:110]","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:19:36.715058Z","iopub.execute_input":"2022-11-22T01:19:36.715501Z","iopub.status.idle":"2022-11-22T01:19:36.724359Z","shell.execute_reply.started":"2022-11-22T01:19:36.715452Z","shell.execute_reply":"2022-11-22T01:19:36.722996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 3 (Take home): **  \nCan you interpret the results above? What do they mean?","metadata":{"id":"l743vmwgYTpo"}},{"cell_type":"markdown","source":"#### Answer here\nHigh training accuracy but low validation accuracy, the cause of the results above is overfitting, the production of an analysis that corresponds too closely or exactly to a particular set of data.\n<br>According to class mentioned, the Decision Tree classifier has issues of either underfitting or overfitting.","metadata":{"id":"8pYICOxsYTpo","execution":{"iopub.status.busy":"2022-10-27T12:06:24.872783Z","iopub.execute_input":"2022-10-27T12:06:24.873919Z","iopub.status.idle":"2022-10-27T12:06:24.880754Z","shell.execute_reply.started":"2022-10-27T12:06:24.873775Z","shell.execute_reply":"2022-10-27T12:06:24.878831Z"}}},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 4 (Take home): **  \nBuild a model using a ```Naive Bayes``` model and train it. What are the testing results? \n\n*Reference*: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html","metadata":{"id":"GaHpgl87YTpo"}},{"cell_type":"code","source":"# Answer here\n\n# for a classificaiton problem, you need to provide both training & testing data\nX_train = BOW_500.transform(train_df['text'])\ny_train = train_df['emotion']\n\nX_test = BOW_500.transform(test_df['text'])\ny_test = test_df['emotion']\n\n## take a look at data dimension is a good habbit  :)\nprint('X_train.shape: ', X_train.shape)\nprint('y_train.shape: ', y_train.shape)\nprint('X_test.shape: ', X_test.shape)\nprint('y_test.shape: ', y_test.shape)","metadata":{"id":"ZPvaHzpXYTpo","execution":{"iopub.status.busy":"2022-11-22T01:22:21.351742Z","iopub.execute_input":"2022-11-22T01:22:21.352186Z","iopub.status.idle":"2022-11-22T01:22:22.319023Z","shell.execute_reply.started":"2022-11-22T01:22:21.352151Z","shell.execute_reply":"2022-11-22T01:22:22.317409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## build and train Naive Bayes model\nfrom sklearn.naive_bayes import MultinomialNB\nNB_model = MultinomialNB()\nNB_model = NB_model.fit(X_train, y_train)\n\n## predict!\ny_train_pred = NB_model.predict(X_train)\ny_test_pred = NB_model.predict(X_test)\n\n## so we get the pred result\ny_test_pred[:10]","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:22:25.306189Z","iopub.execute_input":"2022-11-22T01:22:25.307182Z","iopub.status.idle":"2022-11-22T01:22:25.328979Z","shell.execute_reply.started":"2022-11-22T01:22:25.307139Z","shell.execute_reply":"2022-11-22T01:22:25.327507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## accuracy\nfrom sklearn.metrics import accuracy_score\nacc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\nacc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n\nprint('training accuracy: {}'.format(round(acc_train, 2)))\nprint('testing accuracy: {}'.format(round(acc_test, 2)))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:22:27.593740Z","iopub.execute_input":"2022-11-22T01:22:27.594163Z","iopub.status.idle":"2022-11-22T01:22:27.606616Z","shell.execute_reply.started":"2022-11-22T01:22:27.594133Z","shell.execute_reply":"2022-11-22T01:22:27.605133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## precision, recall, f1-score,\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true=y_test, y_pred=y_test_pred))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:22:30.121006Z","iopub.execute_input":"2022-11-22T01:22:30.121449Z","iopub.status.idle":"2022-11-22T01:22:30.145349Z","shell.execute_reply.started":"2022-11-22T01:22:30.121412Z","shell.execute_reply":"2022-11-22T01:22:30.143713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## check by confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true=y_test, y_pred=y_test_pred) \nprint(cm)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:22:32.695754Z","iopub.execute_input":"2022-11-22T01:22:32.696700Z","iopub.status.idle":"2022-11-22T01:22:32.705654Z","shell.execute_reply.started":"2022-11-22T01:22:32.696658Z","shell.execute_reply":"2022-11-22T01:22:32.704526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 5 (Take home): **  \n\nHow do the results from the Naive Bayes model and the Decision Tree model compare? How do you interpret these differences? Use the theoretical background covered in class to try and explain these differences.","metadata":{"id":"Xv2DqWQSYTpo"}},{"cell_type":"markdown","source":"#### Answer here\n\nThe Decision Tree model performs a better training accuracy but worse testing accuracy than Naive Bayes model, due to it has a issue of overfitting.\n<br>Based on the result of two models above, Naive Bayes model gains a better testing accuracy in the end.","metadata":{"id":"ALN_jHdlYTpo","execution":{"iopub.status.busy":"2022-11-01T10:02:55.416168Z","iopub.execute_input":"2022-11-01T10:02:55.416829Z","iopub.status.idle":"2022-11-01T10:02:55.427721Z","shell.execute_reply.started":"2022-11-01T10:02:55.416793Z","shell.execute_reply":"2022-11-01T10:02:55.425767Z"}}},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 6 (Take home): **  \n\nPlot the Training and Validation Accuracy and Loss (different plots), just like the images below (Note: the pictures below are an example from a different model). How to interpret the graphs you got? How are they related to the concept of overfitting/underfitting covered in class?\n\n![image.png](https://raw.githubusercontent.com/keziatamus/DM2022-Lab2-Master/08755efc671824064d7a9347edb8c418550c3e83//pics/pic3.png)![image.png](https://raw.githubusercontent.com/keziatamus/DM2022-Lab2-Master/08755efc671824064d7a9347edb8c418550c3e83//pics/pic4.png)","metadata":{"id":"NoYqY0-tYTp5"}},{"cell_type":"code","source":"# Answer here\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(14, 5))\n\n#plot 1:\nplt.subplot(1, 2, 1)\nplt.plot(training_log.epoch,training_log.accuracy, label=\"accuracy\", color='b', linewidth=1.5)\nplt.plot(training_log.epoch,training_log.val_accuracy, label=\"val_accuracy\", color='r', linewidth=1.5)\nplt.title('Training Accuracy per epoch')\nplt.legend(loc=\"upper right\")\n\n#plot 2:\nplt.subplot(1, 2, 2)\nplt.plot(training_log.epoch,training_log.loss, label=\"loss\", color='b', linewidth=1.5)\nplt.plot(training_log.epoch,training_log.val_loss, label=\"val_loss\", color='r', linewidth=1.5)\nplt.title('Training Loss  per epoch')\nplt.legend(loc=\"upper right\")\n\nplt.show()","metadata":{"id":"AlhstCrlYTp5","execution":{"iopub.status.busy":"2022-11-22T01:24:32.420560Z","iopub.execute_input":"2022-11-22T01:24:32.421890Z","iopub.status.idle":"2022-11-22T01:24:32.791621Z","shell.execute_reply.started":"2022-11-22T01:24:32.421835Z","shell.execute_reply":"2022-11-22T01:24:32.790441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Observation**\n\nThe graph on the left-hand side shows the validation accuracy goes up quickly in the begining, but becomes stagnant after the first few epoches, it seems to be underfitting.\n<br>And the other graph shows both the train loss and the validation loss goes down in the beginning, but after that the validation loss increases instead of decreasing, it's the sign of overfitting.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{"id":"UYabzgSGYTp5"}},{"cell_type":"markdown","source":"### Note\n\nIf you don't have a GPU (level is higher than GTX 1060) or you are not good at setting lots of things about computer, we recommend you to use the [kaggle kernel](https://www.kaggle.com/kernels) to do deep learning model training. They have already installed all the librarys and provided free GPU for you to use.\n\nNote however that you will only be able to run a kernel for 6 hours. After 6 hours of inactivity, your Kaggle kernel will shut down (meaning if your model takes more than 6 hours to train, you can't train it at once).\n\n\n### More Information for your reference\n\n* Keras document: https://keras.io/\n* Keras GitHub example: https://github.com/keras-team/keras/tree/master/examples\n* CS229: Machine Learning: http://cs229.stanford.edu/syllabus.html\n* Deep Learning cheatsheet: https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning\n* If you want to try TensorFlow or PyTorch: https://pytorch.org/tutorials/\nhttps://www.tensorflow.org/tutorials/quickstart/beginner","metadata":{"id":"4e5eiVLOYTp5"}},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 7 (Take home): **  \n\nNow, we have the word vectors, but our input data is a sequence of words (or say sentence). \nHow can we utilize these \"word\" vectors to represent the sentence data and train our model?\n","metadata":{"id":"y3RQVnBOYTp5"}},{"cell_type":"markdown","source":"#### Answer here\n\nCreating the embedding matrix.\n<br>It's a list of lists, the index of list is same as the word index from the tokenizer and each list is the word corresponding verctor.\n<br>The embedding matrix can be used by classification model.","metadata":{"id":"TBwRT93DYTp5","execution":{"iopub.status.busy":"2022-10-27T12:08:05.380353Z","iopub.execute_input":"2022-10-27T12:08:05.381284Z","iopub.status.idle":"2022-10-27T12:08:05.387012Z","shell.execute_reply.started":"2022-10-27T12:08:05.381233Z","shell.execute_reply":"2022-10-27T12:08:05.385582Z"}}},{"cell_type":"markdown","source":"  ","metadata":{"id":"4FeIFzzxYTp5"}},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 8 (Take home): **  \n\nGenerate a t-SNE visualization to show the 15 words most related to the words \"angry\", \"happy\", \"sad\", \"fear\" (60 words total).","metadata":{"id":"2PL61rqYYTp5"}},{"cell_type":"code","source":"# Answer here\n\nword_list = ['angry', 'happy', 'sad', 'fear']\n\ntopn = 15\nangry_words = ['angry'] + [word_ for word_, sim_ in w2v_google_model.most_similar('angry', topn=topn)]   \nhappy_words = ['happy'] + [word_ for word_, sim_ in w2v_google_model.most_similar('happy', topn=topn)]\nsad_words = ['sad'] + [word_ for word_, sim_ in w2v_google_model.most_similar('sad', topn=topn)]        \nfear_words = ['fear'] + [word_ for word_, sim_ in w2v_google_model.most_similar('fear', topn=topn)]        \n\nprint('angry_words: ', angry_words)\nprint('happy_words: ', happy_words)\nprint('sad_words: ', sad_words)\nprint('fear_words: ', fear_words)\n\ntarget_words = angry_words + happy_words + sad_words + fear_words\nprint('\\ntarget words: ')\nprint(target_words)\n\nprint('\\ncolor list:')\ncn = topn + 1\ncolor = ['b'] * cn + ['g'] * cn + ['r'] * cn + ['y'] * cn\nprint(color)","metadata":{"id":"rvh7ymeNYTp5","execution":{"iopub.status.busy":"2022-11-22T01:28:07.187798Z","iopub.execute_input":"2022-11-22T01:28:07.188438Z","iopub.status.idle":"2022-11-22T01:28:08.353346Z","shell.execute_reply.started":"2022-11-22T01:28:07.188402Z","shell.execute_reply":"2022-11-22T01:28:08.351988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{"id":"_fF1woa8YTp5"}},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\n## w2v model\nmodel = w2v_google_model\n\n## prepare training word vectors\nsize = 2000\ntarget_size = len(target_words)\nall_word = list(model.index_to_key)\nword_train = target_words + all_word[:size]\nX_train = model[word_train]\n\n## t-SNE model\ntsne = TSNE(n_components=2, metric='cosine', random_state=28)\n\n## training\nX_tsne = tsne.fit_transform(X_train)\n\n## plot the result\nplt.figure(figsize=(7.5, 7.5), dpi=115)\nplt.scatter(X_tsne[:target_size, 0], X_tsne[:target_size, 1], c=color)\nfor label, x, y in zip(target_words, X_tsne[:target_size, 0], X_tsne[:target_size, 1]):\n    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:28:12.344874Z","iopub.execute_input":"2022-11-22T01:28:12.345276Z","iopub.status.idle":"2022-11-22T01:28:26.139334Z","shell.execute_reply.started":"2022-11-22T01:28:12.345243Z","shell.execute_reply":"2022-11-22T01:28:26.137628Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
