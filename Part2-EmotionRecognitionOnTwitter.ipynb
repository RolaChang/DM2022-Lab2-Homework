{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# select your GPU. Note that this should be set before you load tensorflow or pytorch.\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# To use multiple GPUs, combine all GPU ID with commas\n",
    "# e.g. >>> os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,3'\n",
    "\n",
    "import torch\n",
    "# Check if any GPU is used\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim: 4.1.2\n",
      "tensorflow: 2.7.0\n",
      "keras: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import data_mining_helpers as dmh\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# keras\n",
    "# preprocess\n",
    "import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(\"gensim: \" + gensim.__version__)\n",
    "print(\"tensorflow: \" + tf.__version__)\n",
    "print(\"keras: \" + keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### training data\n",
    "data_identification = pd.read_csv(\"data_identification.csv\") #1867535 rows\n",
    "emotion = pd.read_csv(\"emotion.csv\") #1455563 rows\n",
    "sampleSubmission = pd.read_csv(\"sampleSubmission.csv\") #411972 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_score': 391, '_index': 'hashtag_tweets', '_source': {'tweet': {'hashtags': ['Snapchat'], 'tweet_id': '0x376b20', 'text': 'People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that\\'s <LH>'}}, '_crawldate': '2015-05-23 11:42:47', '_type': 'tweets'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open('tweets_DM.json','r', encoding='utf')\n",
    "data = []\n",
    "for line in f.readlines():\n",
    "    dic = json.loads(line)\n",
    "    data.append(dic)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text\n",
       "0  0x376b20  People who post \"add me on #Snapchat\" must be ...\n",
       "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...\n",
       "2  0x28b412  Confident of your obedience, I write to you, k...\n",
       "3  0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>\n",
       "4  0x2de201  \"Trust is not the same as faith. A friend is s..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {\n",
    "    'tweet_id':[],\n",
    "    'text':[]\n",
    "}\n",
    "for d in data:\n",
    "    for key in dic.keys():\n",
    "        dic[key].append(d['_source']['tweet'][key])\n",
    "\n",
    "tweet_df = pd.DataFrame(dic)\n",
    "tweet_df.head() #1867535 rows Ã— 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## split into train, test dataset\n",
    "\n",
    "df = pd.merge(tweet_df, data_identification)\n",
    "train = df[df['identification']=='train']\n",
    "test = df[df['identification']=='test']\n",
    "train = pd.merge(train, emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy             0.354514\n",
       "anticipation    0.171023\n",
       "trust           0.141167\n",
       "sadness         0.132895\n",
       "disgust         0.095565\n",
       "fear            0.043969\n",
       "surprise        0.033478\n",
       "anger           0.027389\n",
       "Name: emotion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check emotion distribution\n",
    "train['emotion'].value_counts()/len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-20T06:11:25.673448Z",
     "iopub.status.busy": "2022-11-20T06:11:25.672594Z",
     "iopub.status.idle": "2022-11-20T06:11:25.679599Z",
     "shell.execute_reply": "2022-11-20T06:11:25.678595Z",
     "shell.execute_reply.started": "2022-11-20T06:11:25.673399Z"
    }
   },
   "source": [
    "### 1.2 Save data\n",
    "The pickle module implements binary protocols for serializing and de-serializing a Python object structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-20T06:12:04.375683Z",
     "iopub.status.busy": "2022-11-20T06:12:04.375233Z",
     "iopub.status.idle": "2022-11-20T06:12:06.815173Z",
     "shell.execute_reply": "2022-11-20T06:12:06.813865Z",
     "shell.execute_reply.started": "2022-11-20T06:12:04.375647Z"
    }
   },
   "outputs": [],
   "source": [
    "## save to pickle file\n",
    "train.to_pickle(\"train.pkl\") \n",
    "test.to_pickle(\"test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## load a pickle file\n",
    "train = pd.read_pickle(\"train.pkl\")\n",
    "test = pd.read_pickle(\"test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Text preprocessing\n",
    "\n",
    "nltk.TweetTokenizer keeps hashtags intact while nltk.word_tokenize doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import custom emoji to adjectives about emotions mappings\n",
    "emoji_dict = dmh.emoji\n",
    "emojis = emoji_dict.keys()\n",
    "\n",
    "# import custom frequent name to adjectives about emotions mappings\n",
    "frequent_name_dict = dmh.frequent_name_dict\n",
    "frequent_names = frequent_name_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "def lemma_text(text):\n",
    "    text = text.lower()\n",
    "    grams = nltk.word_tokenize(text)\n",
    "    grams = [\"\" if w == \"useruser\" else w for w in grams]\n",
    "    grams = [\"\" if w == \"hashtaghashtag\" else w for w in grams]\n",
    "    text = ' '.join(grams)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def text_preprocess(df):\n",
    "    df['text'] = df['text'].apply(lambda s : s.replace('',''))\n",
    "    df['tmp'] = df['text'].apply(lambda x: tweet_tokenizer.tokenize(x))\n",
    "    \n",
    "    # replace frequent name  with adjectives\n",
    "    df['tmp'] = df['tmp'].apply(lambda u : [frequent_name_dict[w] if w in frequent_names else w for w in u])\n",
    "    \n",
    "    # replace @username with \n",
    "    p = re.compile('@*')\n",
    "    df['tmp'] = df['tmp'].apply(lambda u : [\"useruser\" if p.match(w).span() != (0, 0) else w for w in u])\n",
    "    \n",
    "    # replace #hashtag with \n",
    "    p = re.compile('#*')\n",
    "    df['tmp'] = df['tmp'].apply(lambda u : [\"hashtaghashtag \" + w[1:] if p.match(w).span() != (0, 0) else w for w in u])\n",
    "\n",
    "    # replace common emojis with adjectives\n",
    "    df['tmp'] = df['tmp'].apply(lambda u : [emoji_dict[w] if w in emojis else w for w in u])\n",
    "        \n",
    "    df['clean_text'] = df['tmp'].apply(lambda x : ' '.join(x)) \n",
    "    df.drop(['tmp'], axis=1, inplace=True)  \n",
    "    df['clean_text'] = df['clean_text'].apply(lemma_text)\n",
    "    \n",
    "    # remove digits\n",
    "    df['clean_text'] = df['clean_text'].str.replace('\\d+', '') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_preprocess(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>test</td>\n",
       "      <td>confident of your obedience , i write to you ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>test</td>\n",
       "      <td>`` trust is not the same as faith . a friend i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>test</td>\n",
       "      <td>when do you have enough ? when are you satisfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>test</td>\n",
       "      <td>god woke you up , now chase the day  godsplan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>test</td>\n",
       "      <td>in these tough times , who do you turn to as y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tweet_id                                               text  \\\n",
       "2   0x28b412  Confident of your obedience, I write to you, k...   \n",
       "4   0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "9   0x218443  When do you have enough ? When are you satisfi...   \n",
       "30  0x2939d5  God woke you up, now chase the day #GodsPlan #...   \n",
       "33  0x26289a  In these tough times, who do YOU turn to as yo...   \n",
       "\n",
       "   identification                                         clean_text  \n",
       "2            test  confident of your obedience , i write to you ,...  \n",
       "4            test  `` trust is not the same as faith . a friend i...  \n",
       "9            test  when do you have enough ? when are you satisfi...  \n",
       "30           test  god woke you up , now chase the day  godsplan ...  \n",
       "33           test  in these tough times , who do you turn to as y...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocess(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to pickle file\n",
    "train.to_pickle(\"new_preprocess_train.pkl\") \n",
    "test.to_pickle(\"new_preprocess_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>people who post `` add me on  snapchat `` must...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>as we see , trump is dangerous to  freepress ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "      <td>now issa is stalking tasha &lt; lolface &gt; &lt; lolfa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>thx for the best time tonight . what stories...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>still waiting on those supplies liscus . &lt; lh &gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text identification  \\\n",
       "0  0x376b20  People who post \"add me on #Snapchat\" must be ...          train   \n",
       "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...          train   \n",
       "2  0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>          train   \n",
       "3  0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...          train   \n",
       "4  0x2c91a8       Still waiting on those supplies Liscus. <LH>          train   \n",
       "\n",
       "        emotion                                         clean_text  \n",
       "0  anticipation  people who post `` add me on  snapchat `` must...  \n",
       "1       sadness   as we see , trump is dangerous to  freepress ...  \n",
       "2          fear  now issa is stalking tasha < lolface > < lolfa...  \n",
       "3           joy    thx for the best time tonight . what stories...  \n",
       "4  anticipation    still waiting on those supplies liscus . < lh >  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load a pickle file\n",
    "import pandas as pd\n",
    "train = pd.read_pickle(\"new_preprocess_train.pkl\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>test</td>\n",
       "      <td>confident of your obedience , i write to you ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>test</td>\n",
       "      <td>`` trust is not the same as faith . a friend i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>test</td>\n",
       "      <td>when do you have enough ? when are you satisfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>test</td>\n",
       "      <td>god woke you up , now chase the day  godsplan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>test</td>\n",
       "      <td>in these tough times , who do you turn to as y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tweet_id                                               text  \\\n",
       "2   0x28b412  Confident of your obedience, I write to you, k...   \n",
       "4   0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "9   0x218443  When do you have enough ? When are you satisfi...   \n",
       "30  0x2939d5  God woke you up, now chase the day #GodsPlan #...   \n",
       "33  0x26289a  In these tough times, who do YOU turn to as yo...   \n",
       "\n",
       "   identification                                         clean_text  \n",
       "2            test  confident of your obedience , i write to you ,...  \n",
       "4            test  `` trust is not the same as faith . a friend i...  \n",
       "9            test  when do you have enough ? when are you satisfi...  \n",
       "30           test  god woke you up , now chase the day  godsplan ...  \n",
       "33           test  in these tough times , who do you turn to as y...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load a pickle file\n",
    "import pandas as pd\n",
    "test = pd.read_pickle(\"new_preprocess_test.pkl\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x2c7743</td>\n",
       "      <td>When your friends offer to bring you food ðŸ˜­ðŸ’˜ #...</td>\n",
       "      <td>when your friends offer to bring you food bawl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2c1eed</td>\n",
       "      <td>I've never let any money problems stop me.. I ...</td>\n",
       "      <td>i 've never let any money problems stop me .. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2826ea</td>\n",
       "      <td>@KurtSchlichter Being a Hobby Historian Chelse...</td>\n",
       "      <td>being a hobby historian chelsea , let me say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x356d9a</td>\n",
       "      <td>#CannabisÂ offers a natural alternative treatme...</td>\n",
       "      <td>cannabis offers a natural alternative treatme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x20fd95</td>\n",
       "      <td>Last Friday off before school starts. I'm read...</td>\n",
       "      <td>last friday off before school starts . i 'm re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  \\\n",
       "0  0x2c7743  When your friends offer to bring you food ðŸ˜­ðŸ’˜ #...   \n",
       "1  0x2c1eed  I've never let any money problems stop me.. I ...   \n",
       "2  0x2826ea  @KurtSchlichter Being a Hobby Historian Chelse...   \n",
       "3  0x356d9a  #CannabisÂ offers a natural alternative treatme...   \n",
       "4  0x20fd95  Last Friday off before school starts. I'm read...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  when your friends offer to bring you food bawl...  \n",
       "1  i 've never let any money problems stop me .. ...  \n",
       "2   being a hobby historian chelsea , let me say ...  \n",
       "3   cannabis offers a natural alternative treatme...  \n",
       "4  last friday off before school starts . i 'm re...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## IMPOTANT!!!! Make sure the test index is correct\n",
    "test = pd.merge(sampleSubmission, test, left_on='id', right_on='tweet_id')\n",
    "test = test[['id', 'text', 'clean_text']]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Load pre-train word embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gunzip glove_twitter_27B_200d_txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ok\n",
      "time : 91.3127691745758 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# GloVe twitter\n",
    "start = time.time()\n",
    "#model_path = \"data/glove.twitter.27B/word2vec.twitter.27B.200d.txt\"\n",
    "w2v_twitter_200_model = KeyedVectors.load_word2vec_format('glove_twitter_27B_200d_txt', binary=False)\n",
    "\n",
    "print('load ok')\n",
    "print(f'time : {time.time() - start} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1019e-02 -8.4930e-02 -3.3063e-01  1.1144e-01 -7.9505e-01  1.8700e-01\n",
      "  4.3807e-01  3.5574e-01 -1.6642e-01 -7.2243e-02 -3.4030e-01 -9.5610e-02\n",
      " -9.8300e-01  5.4471e-01 -7.0435e-02 -4.9007e-02  4.2244e-02  5.0077e-02\n",
      "  1.7844e-01 -1.5049e+00  4.7136e-01  1.2922e-02  1.5759e-01  3.7176e-01\n",
      "  2.5396e-01  1.1585e+00 -4.7953e-01  3.4519e-02 -7.7085e-02  1.6277e-01\n",
      "  4.4839e-01  1.3274e-01 -3.8485e-02  6.3778e-02  3.3766e-01 -8.9009e-02\n",
      "  2.9327e-02  5.4011e-01  1.1109e+00  1.3682e-01  4.4503e-01  2.4157e-01\n",
      "  1.2933e-01 -1.2803e-01  4.7938e-01  3.8640e-02  2.9838e-01  6.6712e-01\n",
      "  1.9765e-01 -7.8086e-01 -1.2398e-01  4.5755e-01 -9.3772e-02 -5.4343e-01\n",
      " -2.6566e-01 -2.9321e-01 -1.5474e-01  8.1505e-02 -3.5291e-01 -4.4827e-01\n",
      " -1.0661e-01 -2.2966e-01  1.6354e-01  5.0833e-02  1.7096e-02 -3.1755e-02\n",
      " -3.9022e-01  9.8955e-01  1.5476e-03  5.3068e-01 -3.2936e-01  1.1219e-01\n",
      " -3.1221e-01 -3.1554e-01  3.1378e-01 -3.1787e-01  5.6004e-01  7.0350e-01\n",
      "  3.4807e-01  3.9264e-01 -2.2543e-01  1.6285e-01  9.0167e-01 -3.4905e-01\n",
      "  2.9596e-01 -6.9033e-01  3.3426e-01  4.6550e-01 -3.6009e-01 -3.9900e-01\n",
      "  5.2167e-02  5.9364e-01 -4.7452e-01 -4.3695e-01  3.0127e-01 -2.4373e-01\n",
      "  3.5973e-01  3.1563e-01  1.7709e-01  1.8776e-01  1.6623e-02 -7.5639e-01\n",
      " -4.7634e-01  1.8489e-01 -2.7393e-02 -4.8064e-01  1.9178e-01 -2.6191e-01\n",
      " -2.6881e-01  4.8657e-01 -1.0986e-01 -6.1295e-01 -5.6986e-01 -5.1311e-01\n",
      "  9.3195e-02  5.5202e-01  4.2237e-01  2.8802e-01  4.5193e-01  6.2112e-01\n",
      " -2.5888e-01 -7.7826e-03  2.7174e-01 -5.1181e-01 -3.6074e-01  9.2003e-02\n",
      " -8.7213e-01 -3.6768e-01  9.5325e-02 -6.4551e-01 -1.8535e-01  3.9449e-01\n",
      "  1.0001e+00  3.7575e-01 -3.8245e-01 -3.4233e-01 -3.2094e-01  4.6989e-01\n",
      " -5.2334e-01 -5.5560e-01  7.9816e-01  4.7343e-02  1.9988e-01  9.3101e-02\n",
      " -2.1135e-01 -7.5002e-02 -2.6250e-01 -1.8432e-01 -3.0797e-02 -2.4506e-02\n",
      " -2.3970e-01 -1.5428e-01 -3.2456e+00  2.1067e-01 -5.0075e-02  5.5089e-01\n",
      "  5.4838e-01 -3.3648e-01 -5.3736e-03 -1.0270e-01  6.4815e-01  1.6708e-01\n",
      "  4.6431e-02 -2.7319e-01 -4.6766e-01 -9.5222e-02  5.3515e-02 -2.0994e-01\n",
      "  1.5147e-01  3.9903e-01  5.1198e-01  2.3426e-02  3.1490e-01 -1.2903e-01\n",
      " -2.2339e-03  5.3658e-01  1.1486e-01 -6.7192e-02 -5.4222e-01 -1.2456e+00\n",
      "  1.8587e-01  2.0009e-01  7.5983e-02 -1.5225e-01  1.0229e-01  8.5227e-01\n",
      " -5.2343e-01 -1.3133e-01  9.3670e-03 -3.7917e-01  2.3225e-01  4.6718e-01\n",
      " -4.4593e-02  6.3988e-02  2.3558e-01  2.9894e-01 -2.6993e-02  8.9193e-01\n",
      " -2.1757e-01 -2.9224e-01]\n"
     ]
    }
   ],
   "source": [
    "vector = w2v_twitter_200_model['language']  # Get numpy vector of a word\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('college', 0.796126127243042),\n",
       " ('class', 0.761738657951355),\n",
       " ('tomorrow', 0.7286202311515808),\n",
       " ('high', 0.7116101384162903),\n",
       " ('kids', 0.7063326239585876)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### print top 5 most similar words to \"school\"\n",
    "w2v_twitter_200_model.most_similar('school', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX = 50000\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "<ipython-input-7-6f8918cfc467>:6: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  tok.fit_on_texts(pd.concat([train,test],ignore_index=True)['clean_text'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508123\n"
     ]
    }
   ],
   "source": [
    "## Give each word a unique number.\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tok = Tokenizer(num_words=MAX)\n",
    "tok.fit_on_texts(pd.concat([train,test],ignore_index=True)['clean_text'])\n",
    "#tok.fit_on_texts(train['clean_text'].values) \n",
    "print(len(tok.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109, 95)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convert phrases into numbers.\n",
    "\n",
    "train_encoded_phrase = tok.texts_to_sequences(train['clean_text'])\n",
    "test_encoded_phrase = tok.texts_to_sequences(test['clean_text'])\n",
    "len(max(train_encoded_phrase, key=len)), len(max(test_encoded_phrase, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1455563, 30) (411972, 30)\n"
     ]
    }
   ],
   "source": [
    "## Padding\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(train_encoded_phrase, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(test_encoded_phrase, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "print(X_train.shape, X_test.shape) #( ,MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1455563, 8) [[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "## One hot encode the labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "enc = label_encoder.fit_transform(train['emotion'])\n",
    "y_train = to_categorical(enc)\n",
    "\n",
    "print(y_train.shape, y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the embedding matrix\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "word_model = w2v_twitter_200_model\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in tok.word_index.items():\n",
    "    if word in word_model.index_to_key:  \n",
    "        # The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\n",
    "        # Use KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\n",
    "        embedding_matrix[i] = word_model[word]\n",
    "    #else continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1164450, 30) (1164450, 8)\n",
      "(291113, 30) (291113, 8)\n"
     ]
    }
   ],
   "source": [
    "## Split training data into train and validation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2,random_state=42)\n",
    "print(X_train.shape, y_train.shape) #( ,30), ( ,8)\n",
    "print(X_val.shape, y_val.shape)     #( ,30), ( ,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('new_embedding_matrix.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "loaded_embedding_matrix = np.load('new_embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(508124, 200)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_embedding_matrix.shape #(len(word_index) 508123+1, EMBEDDING_DIM 200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Embedding, LSTM, ReLU, Dropout, Bidirectional ,Flatten #,  Convolution2D\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "embedding_layer = Embedding(vocab_size, \n",
    "                            EMBEDDING_DIM, \n",
    "                            weights=[loaded_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, \n",
    "                            trainable=False)\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "model.add(embedding_layer)\n",
    "#model.add(Embedding(input_dim=vocab_size,\n",
    "#                    output_dim=EMBEDDING_DIM,\n",
    "#                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                    embeddings_initializer=Constant(loaded_embedding_matrix))) \n",
    "hidden_nodes = 192\n",
    "#model.add(CuDNNLSTM(hidden_nodes))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "#model.add(LSTM(64,return_sequences=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8,activation='sigmoid')) \n",
    "#model.add(Dense(8,activation='softmax')) \n",
    "#model.compile(optimizer=RMSprop(learning_rate=1e-3),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.compile(optimizer=Adam(learning_rate=0.01),loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 30, 200)           101624800 \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 30, 128)          135680    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,860,328\n",
      "Trainable params: 235,528\n",
      "Non-trainable params: 101,624,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "36390/36390 [==============================] - 601s 16ms/step - loss: 1.2676 - accuracy: 0.5383 - val_loss: 1.2417 - val_accuracy: 0.5470\n",
      "Epoch 2/8\n",
      "36390/36390 [==============================] - 598s 16ms/step - loss: 1.2260 - accuracy: 0.5534 - val_loss: 1.2260 - val_accuracy: 0.5529\n",
      "Epoch 3/8\n",
      "36390/36390 [==============================] - 599s 16ms/step - loss: 1.2190 - accuracy: 0.5560 - val_loss: 1.2219 - val_accuracy: 0.5555\n",
      "Epoch 4/8\n",
      "36390/36390 [==============================] - 600s 16ms/step - loss: 1.2092 - accuracy: 0.5601 - val_loss: 1.2118 - val_accuracy: 0.5578\n",
      "Epoch 5/8\n",
      "36390/36390 [==============================] - 600s 16ms/step - loss: 1.2065 - accuracy: 0.5614 - val_loss: 1.2126 - val_accuracy: 0.5590\n",
      "Epoch 6/8\n",
      "36390/36390 [==============================] - 602s 17ms/step - loss: 1.2214 - accuracy: 0.5574 - val_loss: 1.2243 - val_accuracy: 0.5555\n",
      "Epoch 7/8\n",
      "36390/36390 [==============================] - 603s 17ms/step - loss: 1.2126 - accuracy: 0.5589 - val_loss: 1.2182 - val_accuracy: 0.5578\n",
      "Epoch 8/8\n",
      "36390/36390 [==============================] - 603s 17ms/step - loss: 1.2091 - accuracy: 0.5610 - val_loss: 1.2125 - val_accuracy: 0.5586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c94374e20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train\n",
    "\n",
    "epochs = 8\n",
    "batch_size = 32\n",
    "\n",
    "model.fit(X_train,y_train,validation_data=(X_val,y_val), epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9098/9098 [==============================] - 66s 7ms/step\n",
      "[[2.51324065e-02 2.06384599e-01 9.88324136e-02 ... 1.99031815e-01\n",
      "  1.29811347e-01 3.09721053e-01]\n",
      " [1.25599012e-01 4.47985113e-01 3.14357132e-01 ... 3.67616117e-01\n",
      "  2.46505499e-01 4.41577822e-01]\n",
      " [1.41671836e-01 4.95341837e-01 2.84560323e-01 ... 4.43956703e-01\n",
      "  2.25281477e-01 3.70145708e-01]\n",
      " ...\n",
      " [2.12457046e-01 1.14287704e-01 3.62575322e-01 ... 5.45460522e-01\n",
      "  1.10623114e-01 1.72504291e-01]\n",
      " [5.19969035e-04 1.88773144e-02 3.09448666e-03 ... 4.93540149e-03\n",
      "  2.29004095e-03 1.78789068e-02]\n",
      " [2.03558281e-01 3.92129421e-01 5.46513438e-01 ... 6.72288358e-01\n",
      "  3.34591180e-01 2.94853836e-01]]\n"
     ]
    }
   ],
   "source": [
    "## predict on validation set\n",
    "\n",
    "pred = model.predict(X_val, batch_size=32)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode prediction results into labels\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_pred = label_decode(label_encoder, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['surprise' 'joy' 'joy' ... 'sadness' 'joy' 'disgust']\n"
     ]
    }
   ],
   "source": [
    "y_val = label_decode(label_encoder, y_val)\n",
    "print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291113 291113\n",
      "0.5586353065648048\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "\n",
    "correct_count = 0\n",
    "for idx, emotion in enumerate(y_pred):\n",
    "    if emotion == y_val[idx]:\n",
    "        correct_count += 1\n",
    "print(len(y_val), len(y_pred))\n",
    "print(correct_count/len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Prediction on public test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12875/12875 [==============================] - 91s 7ms/step\n",
      "[[0.01991662 0.23562595 0.04275774 ... 0.14225954 0.05078214 0.57360375]\n",
      " [0.02553182 0.4316085  0.08516142 ... 0.18353505 0.09098322 0.41320962]\n",
      " [0.09122821 0.06337459 0.545122   ... 0.55441225 0.13822158 0.09522866]\n",
      " ...\n",
      " [0.2049925  0.5078567  0.51867694 ... 0.5815341  0.28922695 0.2856254 ]\n",
      " [0.0320158  0.58817166 0.10578525 ... 0.16094169 0.10285322 0.59728754]\n",
      " [0.17390262 0.24875572 0.28943646 ... 0.39268878 0.13297367 0.20841748]]\n"
     ]
    }
   ],
   "source": [
    "## predict\n",
    "\n",
    "pred = model.predict(X_test, batch_size=32)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode prediction results into labels\n",
    "y_pred = label_decode(label_encoder, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x2c7743</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2c1eed</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2826ea</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x356d9a</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x20fd95</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  emotion\n",
       "0  0x2c7743      joy\n",
       "1  0x2c1eed      joy\n",
       "2  0x2826ea  sadness\n",
       "3  0x356d9a      joy\n",
       "4  0x20fd95      joy"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform into dataframe to submit to kaggle\n",
    "\n",
    "sampleSubmission['emotion'] = y_pred\n",
    "sampleSubmission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSubmission.to_csv('submission9.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Classification with pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Convert dataframe to Dataset\n",
    "train = Dataset.from_pandas(train[['text', 'emotion']])\n",
    "train = train.remove_columns(\"__index_level_0__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'emotion'],\n",
      "    num_rows: 1455563\n",
      "})\n",
      "{'text': ['People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that\\'s <LH>', '@brianklaas As we see, Trump is dangerous to #freepress around the world. What a <LH> <LH> #TrumpLegacy.  #CNN', 'Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>', '@RISKshow @TheKevinAllison Thx for the BEST TIME tonight. What stories! Heartbreakingly <LH> #authentic #LaughOutLoud good!!', 'Still waiting on those supplies Liscus. <LH>'], 'emotion': ['anticipation', 'sadness', 'fear', 'joy', 'anticipation']}\n"
     ]
    }
   ],
   "source": [
    "print(train)\n",
    "print(train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model you want to use. Available models can be found here: https://huggingface.co/models\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-emotion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare a new encoder and let it learn from the dataset\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "labels = encoder.fit(np.array(train['emotion']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['anticipation']\n",
      " ['sadness']\n",
      " ['fear']\n",
      " ...\n",
      " ['joy']\n",
      " ['joy']\n",
      " ['joy']]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(train['emotion']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# check if still have 6 labels\n",
    "LABEL_COUNT = len(encoder.categories_[0])\n",
    "print(LABEL_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.transform(np.array(train['emotion']).reshape(-1,1)).toarray()\n",
    "#encoder.inverse_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e541c6e2a24db28048e5af2d9f351c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1456 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(dataslice):\n",
    "    \n",
    "    out = tokenizer(dataslice['text'])\n",
    "    out['label'] = encoder.transform(np.array(dataslice['emotion']).reshape(-1,1)).toarray()\n",
    "    return out\n",
    "\n",
    "train = train.map(preprocess, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'emotion', 'input_ids', 'attention_mask', 'label'],\n",
      "    num_rows: 1455563\n",
      "})\n",
      "{'text': 'People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that\\'s <LH>', 'emotion': 'anticipation', 'input_ids': [0, 4763, 54, 618, 22, 4917, 162, 15, 849, 41513, 29465, 113, 531, 28, 36410, 8358, 4, 230, 4987, 313, 17220, 14, 18, 28696, 574, 725, 15698, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "print(train)\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([4, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=8, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('Roberta_dataset.pickle', 'rb') as data:\n",
    "    train = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_val_dataset = train.train_test_split(train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "OUTPUT_DIR = '/home/nlplab/rola/DM/Roberta3'\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 8\n",
    "EPOCH = 1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = OUTPUT_DIR,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    num_train_epochs = EPOCH,\n",
    "    save_steps=100000,\n",
    "    # you can set more parameters here if you want\n",
    ")\n",
    "\n",
    "# now give all the information to a trainer\n",
    "trainer = Trainer(\n",
    "    # set your parameters here\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = train_val_dataset['train'],\n",
    "    eval_dataset = train_val_dataset['test'],\n",
    "    tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 24 18:20:09 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| 47%   36C    P2    97W / 350W |   2888MiB / 12053MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1800276      C   /usr/bin/python3                 2885MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: emotion, text.\n",
      "***** Running training *****\n",
      "  Num examples = 1164450\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 145557\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='145557' max='145557' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [145557/145557 1:20:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.287800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.262400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.258000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.243700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.236300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.231500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.229600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.221400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.221200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.219800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.214300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.217200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.218400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.216400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.209900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.216900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.217300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.210200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.213200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.206300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.204700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.209400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.207800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.203900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.208200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.209100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.202800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.201900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.204700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.205500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.204400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.197500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.201500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.199900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.205200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.208600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.204100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.204100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.198200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.196400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.201400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.195900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.196100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.198800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.198700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.202400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.195700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>0.196700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.201100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.194500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>0.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.191700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>0.195700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.192700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.192500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>0.190900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.193400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>0.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.198400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>0.195100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.194900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>0.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.197600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>0.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.195700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>0.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>0.196300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>0.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.186100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>0.195400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.189600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>0.194900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>0.192600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.189900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>0.195300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>0.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.189900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>0.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>0.189100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>0.191600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.193500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>0.193800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.195400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>0.184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>0.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.191200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>0.186300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.193200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>0.187900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.188700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>0.190700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.191700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>0.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>0.189900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79500</td>\n",
       "      <td>0.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.187300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80500</td>\n",
       "      <td>0.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>0.193400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81500</td>\n",
       "      <td>0.186900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.185200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>0.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>0.193200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83500</td>\n",
       "      <td>0.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.188300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84500</td>\n",
       "      <td>0.187200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.185200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85500</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86500</td>\n",
       "      <td>0.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>0.188200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87500</td>\n",
       "      <td>0.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88500</td>\n",
       "      <td>0.182800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>0.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89500</td>\n",
       "      <td>0.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90500</td>\n",
       "      <td>0.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>0.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91500</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.187200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92500</td>\n",
       "      <td>0.185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>0.192600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93500</td>\n",
       "      <td>0.186500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94500</td>\n",
       "      <td>0.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95500</td>\n",
       "      <td>0.185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.180500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96500</td>\n",
       "      <td>0.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>0.188600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97500</td>\n",
       "      <td>0.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.185600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98500</td>\n",
       "      <td>0.184800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>0.189200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99500</td>\n",
       "      <td>0.184600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100500</td>\n",
       "      <td>0.187700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>0.184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101500</td>\n",
       "      <td>0.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.179900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102500</td>\n",
       "      <td>0.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103000</td>\n",
       "      <td>0.185800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103500</td>\n",
       "      <td>0.184500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.184500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104500</td>\n",
       "      <td>0.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105500</td>\n",
       "      <td>0.192700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.187800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106500</td>\n",
       "      <td>0.189100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107000</td>\n",
       "      <td>0.189600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107500</td>\n",
       "      <td>0.181800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.186300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108500</td>\n",
       "      <td>0.182800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109000</td>\n",
       "      <td>0.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109500</td>\n",
       "      <td>0.185900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110500</td>\n",
       "      <td>0.185800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111000</td>\n",
       "      <td>0.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111500</td>\n",
       "      <td>0.179400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.184600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112500</td>\n",
       "      <td>0.188300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113000</td>\n",
       "      <td>0.183400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113500</td>\n",
       "      <td>0.180600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114500</td>\n",
       "      <td>0.186100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>0.184500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115500</td>\n",
       "      <td>0.183700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.181800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116500</td>\n",
       "      <td>0.183600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117000</td>\n",
       "      <td>0.177900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117500</td>\n",
       "      <td>0.188600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.184100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118500</td>\n",
       "      <td>0.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119000</td>\n",
       "      <td>0.183300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119500</td>\n",
       "      <td>0.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120500</td>\n",
       "      <td>0.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121000</td>\n",
       "      <td>0.183100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121500</td>\n",
       "      <td>0.182600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.183800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122500</td>\n",
       "      <td>0.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123000</td>\n",
       "      <td>0.184100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123500</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.180600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124500</td>\n",
       "      <td>0.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125500</td>\n",
       "      <td>0.188200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126500</td>\n",
       "      <td>0.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127000</td>\n",
       "      <td>0.180600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127500</td>\n",
       "      <td>0.183500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128500</td>\n",
       "      <td>0.183800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129000</td>\n",
       "      <td>0.183500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129500</td>\n",
       "      <td>0.182600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130500</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131000</td>\n",
       "      <td>0.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131500</td>\n",
       "      <td>0.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>0.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132500</td>\n",
       "      <td>0.178500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133000</td>\n",
       "      <td>0.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133500</td>\n",
       "      <td>0.179100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>0.180400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134500</td>\n",
       "      <td>0.185800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>0.187300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135500</td>\n",
       "      <td>0.179900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>0.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136500</td>\n",
       "      <td>0.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137000</td>\n",
       "      <td>0.189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137500</td>\n",
       "      <td>0.178900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>0.180600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138500</td>\n",
       "      <td>0.185300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139000</td>\n",
       "      <td>0.178400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139500</td>\n",
       "      <td>0.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.180500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140500</td>\n",
       "      <td>0.179500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141000</td>\n",
       "      <td>0.187300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141500</td>\n",
       "      <td>0.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>0.186100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142500</td>\n",
       "      <td>0.185300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143000</td>\n",
       "      <td>0.179900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143500</td>\n",
       "      <td>0.181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>0.185600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144500</td>\n",
       "      <td>0.177100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>0.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145500</td>\n",
       "      <td>0.179500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/nlplab/rola/DM/Roberta3/checkpoint-100000\n",
      "Configuration saved in /home/nlplab/rola/DM/Roberta3/checkpoint-100000/config.json\n",
      "Model weights saved in /home/nlplab/rola/DM/Roberta3/checkpoint-100000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/nlplab/rola/DM/Roberta3/checkpoint-100000/tokenizer_config.json\n",
      "Special tokens file saved in /home/nlplab/rola/DM/Roberta3/checkpoint-100000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=145557, training_loss=0.19575007472526418, metrics={'train_runtime': 4846.3519, 'train_samples_per_second': 240.274, 'train_steps_per_second': 30.034, 'total_flos': 2.896544710976688e+16, 'train_loss': 0.19575007472526418, 'epoch': 1.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /home/nlplab/rola/DM/Roberta3/config.json\n",
      "Model weights saved in /home/nlplab/rola/DM/Roberta3/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Saving model for future use\n",
    "\n",
    "model.save_pretrained(save_directory=OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_level ['sadness' 'joy' 'joy' 'joy' 'disgust' 'trust' 'disgust' 'sadness' 'fear'\n",
      " 'joy']\n",
      "label [['sadness']\n",
      " ['joy']\n",
      " ['joy']\n",
      " ['anticipation']\n",
      " ['disgust']\n",
      " ['trust']\n",
      " ['disgust']\n",
      " ['sadness']\n",
      " ['anticipation']\n",
      " ['joy']]\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "#predictions, labels, metrics = trainer.predict(train_val_dataset[\"test\"])\n",
    "\n",
    "predictions = torch.from_numpy(predictions)\n",
    "predictions_after_softmax = nn.functional.softmax(predictions, dim = -1)\n",
    "\n",
    "\n",
    "result = torch.argmax(predictions_after_softmax,dim=1).to(\"cuda:0\")\n",
    "pred_labels = torch.nn.functional.one_hot(result)\n",
    "pred_emotion = encoder.inverse_transform(pred_labels.cpu()).flatten()\n",
    "# get predictions\n",
    "print(\"pred_level\",pred_emotion[:10])\n",
    "print(\"label\",encoder.inverse_transform(labels)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291113\n",
      "291113\n",
      "0.6843081552524277\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "\n",
    "true_emotion = encoder.inverse_transform(train_val_dataset[\"test\"][\"label\"])\n",
    "\n",
    "correct_count = 0\n",
    "for idx, emotion in enumerate(pred_emotion):\n",
    "    if emotion == true_emotion[idx]:\n",
    "        correct_count += 1\n",
    "print(len(pred_emotion))\n",
    "print(len(true_emotion))\n",
    "print(correct_count/len(pred_emotion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model that saved\n",
    "from transformers import AutoModelForSequenceClassification  #DistilBertConfig, DistilBertModel\n",
    "#OUTPUT_DIR = '/home/nlplab/rola/DM/model'\n",
    "configuration = OUTPUT_DIR\n",
    "mymodel = AutoModelForSequenceClassification.from_pretrained(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "test = Dataset.from_pandas(test[['text']])\n",
    "test = test.remove_columns(\"__index_level_0__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'When your friends offer to bring you food ðŸ˜­ðŸ’˜ #loyal #real <LH>'}\n"
     ]
    }
   ],
   "source": [
    "print(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9add1d3e2b2749d0b6f9a5e676f787ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/412 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 411972\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def preprocess_test(dataslice):\n",
    "    \n",
    "    out = tokenizer(dataslice['text'])\n",
    "    return out\n",
    "\n",
    "test = test.map(preprocess_test, batched = True)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 411972\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_level ['joy' 'joy' 'sadness' 'joy' 'joy' 'sadness' 'joy' 'anticipation' 'joy'\n",
      " 'trust']\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "predictions, labels, metrics = trainer.predict(test)\n",
    "predictions = torch.from_numpy(predictions)\n",
    "predictions_after_softmax = nn.functional.softmax(predictions, dim = -1)\n",
    "\n",
    "\n",
    "result = torch.argmax(predictions_after_softmax,dim=1)\n",
    "pred_labels = torch.nn.functional.one_hot(result)\n",
    "pred_emotion = encoder.inverse_transform(pred_labels.cpu()).flatten()\n",
    "\n",
    "print(\"pred_level\",pred_emotion[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# transform into dataframe to submit to kaggle\n",
    "\n",
    "sampleSubmission = pd.read_csv(\"sampleSubmission.csv\")\n",
    "sampleSubmission['emotion'] = pred_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x2c7743</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2c1eed</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2826ea</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x356d9a</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x20fd95</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  emotion\n",
       "0  0x2c7743      joy\n",
       "1  0x2c1eed      joy\n",
       "2  0x2826ea  sadness\n",
       "3  0x356d9a      joy\n",
       "4  0x20fd95      joy"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleSubmission.head()\n",
    "sampleSubmission.to_csv('submission8.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
