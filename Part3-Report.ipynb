{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b849736a",
   "metadata": {},
   "source": [
    "## 1. Raw Data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4431e3a0",
   "metadata": {},
   "source": [
    "First, I transform the tweets_DM.json to pandas dataframe and extract the tweet_id and text part. Second, I merge the dataframe with emotion and data identification go a step further to sperate train/test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3425a",
   "metadata": {},
   "source": [
    "____________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0624043",
   "metadata": {},
   "source": [
    "## 2.  Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3539944e",
   "metadata": {},
   "source": [
    "## 2.1 Word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf09e11-cff1-4aa2-ba6e-80a0c79d2636",
   "metadata": {},
   "source": [
    "### Text Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca241504-f0eb-483f-b9bf-c4e73cab03a8",
   "metadata": {},
   "source": [
    "In this part, I try to do some data preprocessing to increase the cover rate by replacing some tokens to generate a more cleaner data.\n",
    "1. replace common emojis with adjectives\n",
    "2. replace @user token with \"\" token. Also replace #something with \"\" and \"something\" two tokens.\n",
    "3. remove the digits \n",
    "4. convert all text into lowercase\n",
    "5. utilize nltk tweet tokenizer to tokenizer text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c619f",
   "metadata": {},
   "source": [
    "### Load pre-train word embeddings model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0141b7ee",
   "metadata": {},
   "source": [
    "I use pre-trained glove twitter word embedding vectors to generate embedding matrix.\n",
    "<br>source :glove.twitter.27B/word2vec.twitter.27B.200d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4274bf07",
   "metadata": {},
   "source": [
    "### Training data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c11947",
   "metadata": {},
   "source": [
    "1. Use keras.Tokenizer to tokenize text (num_words=50000).\n",
    "2. Use keras.texts_to_sequences to convert phrases into sequence numbers.\n",
    "3. Use keras.pad_sequences to pad the text (maxlen=30).\n",
    "4. Use LabelEncoder to create one-hot encoding for emotion label.\n",
    "5. Create word embedding matrix with pre-trained embeddings.\n",
    "6. Split the data into train set and validation set (test_size=0.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8661192c",
   "metadata": {},
   "source": [
    "### Build model and train with whole data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72804e5d",
   "metadata": {},
   "source": [
    "I try to find the best setting for inner layer, like BidirectionalLSTM, CuDNNLSTM, LSTM, etc.\n",
    "<br>And the best result on Kagglge consist of the following layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5acc24a-b5ea-4ff5-aa86-f4311e731073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Embedding, LSTM, ReLU, Dropout, Bidirectional ,Flatten #,  Convolution2D\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "embedding_layer = Embedding(vocab_size, \n",
    "                            EMBEDDING_DIM, \n",
    "                            weights=[loaded_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, \n",
    "                            trainable=False)\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "model.add(embedding_layer)\n",
    "hidden_nodes = 192\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8,activation='sigmoid')) \n",
    "model.compile(optimizer=Adam(learning_rate=0.01),loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52f47a",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae84e53e-3be5-402b-a207-fa0b21fab8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy\n",
    "\n",
    "correct_count = 0\n",
    "for idx, emotion in enumerate(y_pred):\n",
    "    if emotion == y_val[idx]:\n",
    "        correct_count += 1\n",
    "print(len(y_val), len(y_pred))\n",
    "print(correct_count/len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa7f90c-d07b-48f8-9a38-d58d952d4c31",
   "metadata": {},
   "source": [
    "0.5586353065648048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9abc56-dd37-465e-8a9a-4fb54a794c9a",
   "metadata": {},
   "source": [
    "____________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b72616",
   "metadata": {},
   "source": [
    "## 2.2 roBERTa-base pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b4da0",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a379bb9",
   "metadata": {},
   "source": [
    "1. Convert training data form dataframe to datasets.\n",
    "2. Use roBERTa-base pre-trained model.\n",
    "3. Do sentence processing and label processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ad391",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a82022c-f1b4-4893-8ac4-e124c93b3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "OUTPUT_DIR = '/home/nlplab/rola/DM/Roberta3'\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 8\n",
    "EPOCH = 1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = OUTPUT_DIR,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    num_train_epochs = EPOCH,\n",
    "    save_steps=100000,\n",
    "    # you can set more parameters here if you want\n",
    ")\n",
    "\n",
    "# now give all the information to a trainer\n",
    "trainer = Trainer(\n",
    "    # set your parameters here\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = train_val_dataset['train'],\n",
    "    eval_dataset = train_val_dataset['test'],\n",
    "    tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b726fb",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy\n",
    "\n",
    "true_emotion = encoder.inverse_transform(train_val_dataset[\"test\"][\"label\"])\n",
    "\n",
    "correct_count = 0\n",
    "for idx, emotion in enumerate(pred_emotion):\n",
    "    if emotion == true_emotion[idx]:\n",
    "        correct_count += 1\n",
    "        \n",
    "print(correct_count/len(pred_emotion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc257b1-6ded-41f7-a380-ea72904a8aa6",
   "metadata": {},
   "source": [
    "0.6843081552524277"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
